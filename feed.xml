<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://rishabhdahale.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://rishabhdahale.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-01-06T05:07:10+00:00</updated><id>https://rishabhdahale.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Stochastic Monte Carlo Exploration Starts for POMDPs</title><link href="https://rishabhdahale.github.io/blog/2022/aila-project/" rel="alternate" type="text/html" title="Stochastic Monte Carlo Exploration Starts for POMDPs" /><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>https://rishabhdahale.github.io/blog/2022/aila-project</id><content type="html" xml:base="https://rishabhdahale.github.io/blog/2022/aila-project/">&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This work is carried out by myself and Nitish Tongia under the supervision of Prof. Shivaram Kalyanakrishnan as course project for course &lt;a href=&quot;https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs748-s2021/index.html&quot;&gt;CS 748&lt;/a&gt; at IIT Bombay.&lt;/p&gt;

&lt;p&gt;Monte-Carlo Exploring Starts for POMDP’s (MCESP) is a memory-less, model-free reinforcement learning algorithm that integrates the Monte Carlo exploring starts &lt;a href=&quot;#ref1&quot;&gt;[1]&lt;/a&gt; into a local search of deterministic policy space to perform control tasks in partially observable Markov decision processes (POMDP’s). The novelty in this approach is the introduction of a new action-value function which ensures that the algorithm is theoretically sound and guarantees the convergence to locally optimal policies in some special cases &lt;a href=&quot;#ref2&quot;&gt;[2]&lt;/a&gt;. We implement the algorithm on multiple standard partially observable domains to demonstrate the convergence properties. We then go on to modify the algorithm so that the policy search now works in the space of stochastic policies. We verify the dominance of this modified version empirically over the deterministic version and other stochastic learning algorithms in partially observable domains.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;&lt;a name=&quot;section_intro&quot;&gt;Introduction&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In many realistic situations, the dynamics of the agent’s environment and observations are unknown or only partially known. In such cases, the problem of sequential decision making by intelligent agents is formulated as partially observable Markov decision processes (POMDP). The standard reinforcement learning algorithms such as Q-Learning and Sarsa(\(\lambda\)) ignore the partial observability and treat the observations as the states of a Markov decision problem (MDP). However, the theoretical soundness of such action-value based algorithms is questionable as they can fail to converge even on some very simple problems &lt;a href=&quot;#ref3&quot;&gt;[3]&lt;/a&gt;. Stochastic algorithms that search through a continuous space of stochastic policies by conditioning the action choice on the agent’s immediate observation do actually converge to local optimal policies under appropriate conditions, but data suggests that the learning in these algorithms is much slower as compared to action-value based reinforcement learning algorithms.&lt;/p&gt;

&lt;p&gt;The MCESP algorithm gets the best of both approaches by achieving the better empirical performance of the actionvalue based RL algorithms and provide stronger theoretical properties like the stochastic algorithms. It can be interpreted as a local search algorithm that achieves local optimum in the discrete space of policies that maps observation to actions. The novel action-value function in the algorithm is inspired by the work on fixed-point analysis &lt;a href=&quot;#ref4&quot;&gt;[4]&lt;/a&gt;. Based on different choices of free parameters, different ideas from stochastic optimisation can be incorporated.&lt;/p&gt;

&lt;p&gt;In model-free POMDP literature, the dominance of stochastic policy learning over deterministic policies has been heavily studied and empirically verified in a large number of independent works. In this direction, the modified version of MCESP, which we call Stochastic MCESP, performs the policy search in the space of stochastic policies. This aims to get closer and closer to the hypothetical optimum memory-less stochastic policy.&lt;/p&gt;

&lt;h1 id=&quot;problem-formulation&quot;&gt;&lt;a name=&quot;section_problem_formulation&quot;&gt;Problem Formulation&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;The agent’s environment is modeled as a POMDP with an unknown underlying MDP, but a finite observation set. Based on the trajectory, the agent is required to learn a deterministic policy that maximizes the discounted episodic reward under the assumption that the episodes terminate with certainty under any policy. Later, the search space of policies is extended to a much larger space of stochastic policies. There, the policy search in the continuous space is performed based on the expected value of discounted episodic reward of the trajectory under the stochastic policy.&lt;/p&gt;

&lt;h2 id=&quot;pomdp&quot;&gt;&lt;a name=&quot;subsection_problem_pomdp&quot;&gt;POMDP&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Partially observable markov decision process (POMDP’s) provide a generalised memory-less model for planning under uncertainty represented using \(\{S, A, O, T, \Omega, R, \gamma \}\) where \(S\) is the set of (finite) discrete states, \(A\) is the set of discrete actions, \(O\) is the set of discrete observations providing incomplete/noisy information about state, \(T(s,a,s&apos;) = Pr(s_{t+1}=s&apos;\) | \(s_t=s, a_t=a)\) is the transition probability from state \(s\) to \(s&apos;\) on taking action \(a\), \(\Omega(o, s, a) := Pr(o_{t+1} = o\) | \(a_t = a, s_{t+1} = s)\) is the probability of observing \(o\) from state \(s\) after taking action \(a\), \(R(s,a)\) is the reward obtained on taking action \(a\) from state \(s\) and \(\gamma\) is the discount factor.&lt;/p&gt;

&lt;h1 id=&quot;related-works&quot;&gt;&lt;a name=&quot;section_related_works&quot;&gt;Related Works&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In partially observable domains, using standard RL algorithms which treat the observations as if they were the states of an underlying Markov decision problem (MDP) can lead to sub-optimal behaviour or in the worst case, the parameters learned by the RL algorithm can fail to converge or even diverge &lt;a href=&quot;#ref5&quot;&gt;[5&lt;/a&gt;, &lt;a href=&quot;#ref6&quot;&gt;6]&lt;/a&gt;. The theoretical soundness of such algorithms is questionable if the underlying representation of the environment is not fully markov.&lt;/p&gt;

&lt;p&gt;A large variety of algorithms which perform different kinds of stochastic gradient descent on a fixed error function have been developed &lt;a href=&quot;#ref7&quot;&gt;[7&lt;/a&gt;, &lt;a href=&quot;#ref8&quot;&gt;8&lt;/a&gt;, &lt;a href=&quot;#ref9&quot;&gt;9]&lt;/a&gt;. These usually condition the action choice on immediate observation plus the state of an internal memory. Empirical evidence suggests that these are much slower than the memory-less RL algorithms because of the added complexity of storing and updating history or some sort of belief states.&lt;/p&gt;

&lt;p&gt;The work of &lt;a href=&quot;#ref2&quot;&gt;[2]&lt;/a&gt; on this MCESP algorithm differs from the general approach in this domain by defining a novel action value function which allows us to integrate exploring starts with policy search while still enjoying strong convergence properties. Our contribution of extension of the MCESP algorithm to stochastic policies and the empirical verification of its dominance over standard memory-less approaches like the work done by &lt;a href=&quot;#ref10&quot;&gt;[10]&lt;/a&gt;, to the best of our knowledge, is new.&lt;/p&gt;

&lt;h1 id=&quot;mcesp&quot;&gt;&lt;a name=&quot;section_mcesp&quot;&gt;MCESP&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Refer to Perkins 2002 &lt;a href=&quot;#ref2&quot;&gt;[2]&lt;/a&gt; for more detials of deterministic MCESP.&lt;/p&gt;

&lt;h2 id=&quot;neighbouring-policy&quot;&gt;&lt;a name=&quot;subsection_mcesp_neighbour&quot;&gt;Neighbouring Policy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;One of the key aspect of MCESP algorithm is its interpretation as a local search algorithm in the space of memory-less policies. In that regard, for a policy \(\pi\), define its neighbouring policy as \(\pi \leftarrow (o, a)\) that is identical to \(\pi\) except that it assigns action \(a\) corresponding to observation \(o\).&lt;/p&gt;

&lt;h2 id=&quot;action-value-function&quot;&gt;&lt;a name=&quot;subsection_action_value_func&quot;&gt;Action Value Function&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;For a trajectory \(\tau\), the discounted episodic reward is defined as \(R(\tau) = \sum_{i=0}^{\infty}\gamma^ir_i\). Let an observation \(o\) occurs in trajectory \(\tau\) for the first time at time step \(j\), then we define \(R_{pre-o}(\tau) = \sum_{i=0}^{j-1}\gamma^ir_i\) and \(R_{post-o}(\tau) = \sum_{i=j}^{\infty}\gamma^ir_i\) i.e. the parts of the discounted episodic reward before and after the first occurrence of observation \(o\) respectively. Following this, we define the action value function as the expected post observation return of the neighbouring policy, i.e.&lt;/p&gt;

\[Q^{\pi}_{o,a} = E^{\pi \leftarrow (o,a)}\{R_{post-o(\tau)}\}\]

&lt;p&gt;This definition of action value differs from the one in a standard MDP case in three aspects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The notion of a staring state is replaced by the first occurrence of an observation \(o\)&lt;/li&gt;
  &lt;li&gt;After taking the immediate action \(a\), the agent follows a neighbouring policy \(\pi \leftarrow (o,a)\) of policy \(\pi\)&lt;/li&gt;
  &lt;li&gt;The action value function, instead of being the discounted return that follows \(o\), it is the portion of discounted reward following observation \(o\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This definition of action value function preserves (to some degree) the property of MDP’s that an optimal is necessarily greedy with respect to its action values.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For all \(\pi\) and \(\pi&apos;=\pi \leftarrow (o,a)\)&lt;/p&gt;

\[V^{\pi} + \epsilon \geq V^{\pi&apos;} \iff Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a}\]

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

\[V^{\pi} + \epsilon \geq V^{\pi&apos;}\]

\[\iff E^{\pi}\{R_{pre-o(\tau)}\} + E^{\pi}\{R_{post-o(\tau)}\} + \epsilon\]

\[\geq E^{\pi&apos;}\{R_{pre-o(\tau)}\} + E^{\pi&apos;}\{R_{post-o(\tau)}\}\]

&lt;p&gt;Now, as the expected discounted reward before first occurrence of observation \(o\) is independent of the action taken from \(o \implies E^{\pi}\{R_{pre-o(\tau)}\} = E^{\pi \leftarrow (o,a)}\{R_{pre-o(\tau)}\}\)&lt;/p&gt;

\[\iff  E^{\pi}\{R_{post-o(\tau)}\} + \epsilon \geq E^{\pi \leftarrow (o,a)}\{R_{post-o(\tau)}\}\]

\[\iff  Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a}\]

&lt;p&gt;Following these definitions, we say that a policy \(\pi\) is an \(\epsilon\) locally optimal policy if and only if its action value is at-least \(\epsilon\) beter than all its neighbouring policies, i.e. it satisfies \(Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a} \forall\) observations \(o\) and actions \(a\).&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;&lt;a name=&quot;subsection_algorithm&quot;&gt;Algorithm&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/mcesp.png&quot; alt=&quot;MCESP Algorithm&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The algorithm itself can be seen as two independent parts, first between lines (4-7) and second between lines (8-12). Since the environment is only partially observable to the agent, given a policy, the action values themselves are an estimate of the true values as they are approximated using discounted reward of a finite number of trajectories. The first part of the algorithm works on bringing these action value estimates closer and closer to their true values by performing first visit Monte-Carlo updates for each selected pair of observations and actions. Based on the present estimates, the second part of the algorithm performs a policy search and tries to find neighbouring policy which is significantly better (at-least \(\epsilon\) better) than the current policy. This threshold is dependent on the number of policy updates (n) and the number of action-value updates for the selected observation-action pair (\(c_{o,a}\)) after the last policy update.&lt;/p&gt;

&lt;p&gt;Based on different observation-action pair selection methods (uniformly at random or in round robin fashion), different learning rate schedules (\(\alpha\)) and different choices for comparison threshold (\(\epsilon\)) functions, different variants of MCSEP can be obtained which differ in their performance, computational complexity and convergence properties. We present three such variations each one of which integrate the MCESP algorithm to incorporate different ideas from stochatic optimisation literature.&lt;/p&gt;

&lt;h2 id=&quot;mcesp-saa&quot;&gt;&lt;a name=&quot;subsection_mcesp_saa&quot;&gt;MCESP SAA&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;If the dynamics of the POMDP are unknown to the agent, exact evaluation of any policy is not possible. By generating a fixed number of trajectories(k) under a given policy, an estimate of the policy’s value can be obtained as the average discounted return. This approach is known as Sample Average Approximation &lt;a href=&quot;#ref11&quot;&gt;[11]&lt;/a&gt; in stochastic optimization literature. For larger values of k, the action-value estimates are expected to be accurate and the algorithm is expected to converge to the local-optimal solution.&lt;/p&gt;

&lt;p&gt;\(\alpha(n,i) = \dfrac{1}{i + 1}\) and 
\(\epsilon(n,i,j) = \begin{cases} \infty &amp;amp; i &amp;lt; k \hspace{2mm} or \hspace{2mm} j &amp;lt; k \\ 0 &amp;amp; otherwise \end{cases}\)&lt;/p&gt;

&lt;h2 id=&quot;mcesp-palo&quot;&gt;&lt;a name=&quot;subsection_mcesp_palo&quot;&gt;MCESP PALO&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;In finite local neighborhood structure, PALO &lt;a href=&quot;#ref12&quot;&gt;[12]&lt;/a&gt; algorithm is a general method for hill-climbing in the solution space of a stochastic optimization problem. In MCESP-PALO, the decaying learning rate corresponds to simple averaging, and the comparison threshold is based on Hoeffding’s inequality. The algorithm strictly moves towards the local optimal policy and is expected to converge with absolute certainty.&lt;/p&gt;

\[\epsilon(n,i,j) =  \begin{cases} 
(y-x)\sqrt{\dfrac{1}{2i}ln\bigg(\dfrac{2(k_n-1)N)}{\delta_n}\bigg)} &amp;amp; i = j &amp;lt; k_n \\
\dfrac{\epsilon}{2} &amp;amp; i = j = k_n \\
\infty &amp;amp; otherwise\\
\end{cases}\]

&lt;p&gt;\(k_n = \left\lceil {2\dfrac{(y-x)^{2}}{\epsilon^{2}} ln\Big(\dfrac{2N}{\delta_{n}}\Big)}\right\rceil\) ,
\(\delta_{n} = \dfrac{6\delta}{n^{2}\pi^{2}}\) , 
\(\alpha(n,i) = \dfrac{1}{i + 1}\)&lt;/p&gt;

&lt;h2 id=&quot;mcesp-ce&quot;&gt;&lt;a name=&quot;subsection_mcesp_ce&quot;&gt;MCESP CE&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Since the action values are stochastic, after any finite number of steps, it is not possible to achieve local optimal policy with absolute certainty. However, it serves good as an indicator of non-optimal policy and thereby suggesting a switch. This ensures that the algorithm with a constant comparison threshold will surely converge to at least a \(\epsilon\)-locally optimal policy. Here, \(\alpha(n,i) = b \times i^{-p}\) and \(\epsilon(n,i,j) = \epsilon_0\)&lt;/p&gt;

&lt;p&gt;MCESP-SAA and MCESP-PALO are much more heavier in terms of computational complexity than MCESP-CE. So, due to resource constraints, we kept our focus on producing results for the constant-epsilon version of our algorithm. We took a couple of tasks (one episodic and one continuing) for testing the theoretical claims and verifying their correctness in general. In both of these, the state of the environment was only partially observable to the agent.&lt;/p&gt;

&lt;h1 id=&quot;various-partially-observable-domains&quot;&gt;&lt;a name=&quot;section_mcesp_partial_domains&quot;&gt;Various Partially Observable Domains&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;parr-and-russells-grid-world&quot;&gt;&lt;a name=&quot;subsection_gridworld&quot;&gt;Parr and Russell’s Grid World&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/pnr.png&quot; alt=&quot;Parr and Russell&apos;s Gridworld&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Parr and Russell’s Grid World &lt;a href=&quot;#ref13&quot;&gt;[13]&lt;/a&gt; is a \(4\times 3\) grid consisting of 11 states and an obstacle state as shown in Figure 1. The environment is only partially observable to the agent as it can sense only the presence of walls to its immediate east and west and whether it is in the goal (+1) or the penalty (-1) state. The agent can take 4 actions for movement in North, South, East and West directions and its moves in the desired direction with probability 0.8 and slips to either sides with probability 0.1 each. A constant reward of -0.04 is received by the agent for each transition except for the transition to terminal states, where it receives +1 for reaching goal state and -1 for reaching penalty state. In this partially observable setting, the agent is required to learn the best memory-less policy which maximises the discounted episodic reward from the start state to the terminal state.&lt;/p&gt;

&lt;h2 id=&quot;chrismans-space-shuttle-docking-problem&quot;&gt;&lt;a name=&quot;subsection_shuttle&quot;&gt;Chrisman’s Space Shuttle Docking Problem&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/shuttle.png&quot; alt=&quot;Chrisman&apos;s Space Shuttle Docking Problem&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Chrisman’s Space Shuttle Docking Problem &lt;a href=&quot;#ref14&quot;&gt;[14]&lt;/a&gt; comprises of two stations as shown in Figure 2. The stations are observable to the agent as least recently visited (LRV) or most recently visited (MRV) based on the most recent docking of the shuttle to one of the stations. The agent can sense only 5 different observations in this 8 state POMDP. It can observe as being in front of or being docked to one of the stations(MRV or LRV), and seeing nothing in front. Out of the three actions that agent can take, two (go-forward and turn-around) are deterministic whereas action backup succeed with probability less than 1. Action backup has different effects based  on the true state of the shuttle. If the true state of the shuttle is the middle space, then action backup succeeds with probability 0.8, no effect with probability 0.1 and the effect of turn-around with probability 0.1. If the shuttle is in front of one of the stations with its back facing towards it, action backup will result in successful docking with probability 0.7 and no effect with probability 0.3. If the shuttle is already docked into one of the stations, action backup will cause the the station to deterministically change to the MRV station. If the shuttle is facing one of the stations, then action backup results in successful docking with probability 0.3, results in no effect with probability 0.4 and the effect of turn-around with probability 0.3. As the name suggests, action go-forward propels the shuttle in the forward direction and action turn-around rotates the shuttle by 180 degrees, i.e. reverse the direction of the shuttle. A reward of +10 is awarded for successful docking into the LRV (using action backup), a penalty of -3 is incurred for bumping into the station (taking action go-forward while facing the station). A constant penalty of -0.004 is incurred to incentivise the agent to perform the task as quickly as possible. The goal for the agent is to learn a memory-less deterministic policy that continuously docks to the two stations in an alternate manner.&lt;/p&gt;

&lt;h1 id=&quot;stochastic-mcesp&quot;&gt;&lt;a name=&quot;section_stichastic_mcesp&quot;&gt;Stochastic MCESP&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;The dominance of stochastic policies over deterministic ones has been empirically proven in a large number of independent works and is widely accepted in pomdp literature with respect to memory-less policies. In this regard, we modified the MCESP algorithm to perform the policy search in the continuous space of stochastic policies while still maintaining the core structure of the algorithm. It differs from the deterministic version in two key aspects. First, the neighbouring policy \(\pi \leftarrow (o,a)\) is now defined on the basis of a little perturbation (\(\delta\)) in the conditional probability for an action \(a\) given an observation \(o\). And second, the policy search step is now based on the comparison between the expected values of the action value functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/mcesp_stoch.png&quot; alt=&quot;Stochastic MCESP&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where \(\pi \leftarrow (o,a) = Normalised(\pi&apos;)\) and
\(\pi&apos;(a&apos;|o&apos;) = \begin{cases} 
\hspace{10pt} \pi(a&apos;|o&apos;) &amp;amp; if \hspace{7pt} (o&apos;,a&apos;) \neq (o,a)\\
\pi(a&apos;|o&apos;)+\delta &amp;amp; \hspace{10pt} otherwise\\
\end{cases}\)&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;&lt;a name=&quot;section_results&quot;&gt;Results&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;The plots below shows the comparison between the deterministic algorithm (MCESP-CS), the stochastic algorithm (Williams and Singh &lt;a href=&quot;#ref10&quot;&gt;[10]&lt;/a&gt;) and our modified algorithm (Stochastic MCESP) for Parr and Russell Grid World and Chrisman’s Shuttle problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/pnr_comparison.png&quot; alt=&quot;Stochastic MCESP&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/aila/shuttle_comparison.png&quot; alt=&quot;Stochastic MCESP&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see from both the plots that our modified algorithm dominates both the deterministic version of MCESP and the algorithm by Williams and Singh in terms of value of the final optimal policy when restricted to same amount of training (by fixing the number of interactions with the environment). We can observe that the algorithm by Williams and Singh achieves convergence earlier, but it is nowhere near to the optimal value to which stochastic MCESP converges. Also, the amount of tuning and time taken by Willams and Singh’s is much more than by both the versions of MCESP.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We begin with presenting a novel algorithm MCESP, a model-free memory-less reinforcement learning algorithm for POMDP’s. We ran different variants of the algorithm on various benchmarks &lt;a href=&quot;#ref15&quot;&gt;[15]&lt;/a&gt; in partially observable domains and verify the theoretical claims and convergence properties. The major drawback in our algorithm is that a lot of times, we found that the algorithm is converging to a locally and not globally optimal policy. That is the trade off that the algorithm has in order to enjoy such theoretical guarantees. We focused on one episodic and one continuous task. This can be further extended to see how these claims generalize for much more complex domains.&lt;/p&gt;

&lt;p&gt;Since we know that memory-less stochastic policies in general work much better than deterministic ones for POMDP’s, we modified our algorithm so that now it performs the policy search in the space of stochastic policies without perturbing the core structure of the algorithm. We then compare this with the deterministic version and other stochastic learning algorithms to verify its superior performance. Still, our work is by no means completely refined in all its aspects. Several improvements might be possible like using complex strategies for exploration techniques instead of simple round-robin or selection observation-action uniformly at random.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;a name=&quot;section_reference&quot;&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a name=&quot;ref1&quot;&gt;1.&lt;/a&gt; S. Sutton, R., and G. Barto, A. 2014. Reinforcement learning: An introduction second edition: 2014, 2015.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref2&quot;&gt;2.&lt;/a&gt; Perkins, T. J. 2002. Reinforcement learning for pomdps based on action values and stochastic optimization. 199–204.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref3&quot;&gt;3.&lt;/a&gt; Gordon, G. J. 1996. Chattering in sarsa(lambda) - a cmu learning lab internal report.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref4&quot;&gt;4.&lt;/a&gt; Pendrith, M. D., and Mcgarity, M. J. 1998. An analysis of direct reinforcement learning in non-markovian domains. 421–429.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref5&quot;&gt;5.&lt;/a&gt; Whitehead, S. 1992. Reinforcement learning for the adaptive control of perception and action.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref6&quot;&gt;6.&lt;/a&gt; Baird, L. 1995. Residual algorithms: Reinforcement learning with function approximation. 30–37.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref7&quot;&gt;7.&lt;/a&gt; Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref8&quot;&gt;8.&lt;/a&gt; Baird, L., and Moore, A. 1998. Gradient descent for general reinforcement learning. 968–974.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref9&quot;&gt;9.&lt;/a&gt; Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.; et al. 2000. Policy gradient methods for reinforcement learning with function approximation. 99:1057–1063.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref10&quot;&gt;10.&lt;/a&gt; Williams, J. K., and Singh, S. 1999. Experimental results on learning stochastic memoryless policies for partially observable markov decision processes. 1073–1079.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref11&quot;&gt;11.&lt;/a&gt; Kleywegt, A. J.; Shapiro, A.; and Homem-de Mello, T. 2002. The sample average approximation method for stochastic discrete optimization. SIAM J. on Optimization 12(2):479–502.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref12&quot;&gt;12.&lt;/a&gt; Greiner, R. 1995. Palo: A probabilistic hillclimbing algorithm. Artificial Intelligence 83:1–2.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref13&quot;&gt;13.&lt;/a&gt; Parr, R., and Russell, S. 1995. Approximating optimal policies for partially observable stochastic domains. 95:1088–1094.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref14&quot;&gt;14.&lt;/a&gt; Chrisman, L. 1992. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. 1992:183–188&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ref15&quot;&gt;15.&lt;/a&gt; Cassandra, A. R. 2003. Pomdp examples. Available at &lt;a href=&quot;http://www.pomdp.org/examples/&quot;&gt;pomdp.org&lt;/a&gt;&lt;/p&gt;

&lt;!-- &lt;a name=&quot;ref16&quot;&gt;16.&lt;/a&gt;  --&gt;

&lt;p&gt;&lt;strong&gt;To cite this article:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{smcesp2021,
  title={Stochastic Monte Carlo Exploration Starts for POMDPs},
  author={Dahale, Rishabh and Tongia, Nitish},
  year={2021}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">NOTE: This work is carried out by myself and Nitish Tongia under the supervision of Prof. Shivaram Kalyanakrishnan as course project for course CS 748 at IIT Bombay. Monte-Carlo Exploring Starts for POMDP’s (MCESP) is a memory-less, model-free reinforcement learning algorithm that integrates the Monte Carlo exploring starts [1] into a local search of deterministic policy space to perform control tasks in partially observable Markov decision processes (POMDP’s). The novelty in this approach is the introduction of a new action-value function which ensures that the algorithm is theoretically sound and guarantees the convergence to locally optimal policies in some special cases [2]. We implement the algorithm on multiple standard partially observable domains to demonstrate the convergence properties. We then go on to modify the algorithm so that the policy search now works in the space of stochastic policies. We verify the dominance of this modified version empirically over the deterministic version and other stochastic learning algorithms in partially observable domains. Introduction In many realistic situations, the dynamics of the agent’s environment and observations are unknown or only partially known. In such cases, the problem of sequential decision making by intelligent agents is formulated as partially observable Markov decision processes (POMDP). The standard reinforcement learning algorithms such as Q-Learning and Sarsa(\(\lambda\)) ignore the partial observability and treat the observations as the states of a Markov decision problem (MDP). However, the theoretical soundness of such action-value based algorithms is questionable as they can fail to converge even on some very simple problems [3]. Stochastic algorithms that search through a continuous space of stochastic policies by conditioning the action choice on the agent’s immediate observation do actually converge to local optimal policies under appropriate conditions, but data suggests that the learning in these algorithms is much slower as compared to action-value based reinforcement learning algorithms. The MCESP algorithm gets the best of both approaches by achieving the better empirical performance of the actionvalue based RL algorithms and provide stronger theoretical properties like the stochastic algorithms. It can be interpreted as a local search algorithm that achieves local optimum in the discrete space of policies that maps observation to actions. The novel action-value function in the algorithm is inspired by the work on fixed-point analysis [4]. Based on different choices of free parameters, different ideas from stochastic optimisation can be incorporated. In model-free POMDP literature, the dominance of stochastic policy learning over deterministic policies has been heavily studied and empirically verified in a large number of independent works. In this direction, the modified version of MCESP, which we call Stochastic MCESP, performs the policy search in the space of stochastic policies. This aims to get closer and closer to the hypothetical optimum memory-less stochastic policy. Problem Formulation The agent’s environment is modeled as a POMDP with an unknown underlying MDP, but a finite observation set. Based on the trajectory, the agent is required to learn a deterministic policy that maximizes the discounted episodic reward under the assumption that the episodes terminate with certainty under any policy. Later, the search space of policies is extended to a much larger space of stochastic policies. There, the policy search in the continuous space is performed based on the expected value of discounted episodic reward of the trajectory under the stochastic policy. POMDP Partially observable markov decision process (POMDP’s) provide a generalised memory-less model for planning under uncertainty represented using \(\{S, A, O, T, \Omega, R, \gamma \}\) where \(S\) is the set of (finite) discrete states, \(A\) is the set of discrete actions, \(O\) is the set of discrete observations providing incomplete/noisy information about state, \(T(s,a,s&apos;) = Pr(s_{t+1}=s&apos;\) | \(s_t=s, a_t=a)\) is the transition probability from state \(s\) to \(s&apos;\) on taking action \(a\), \(\Omega(o, s, a) := Pr(o_{t+1} = o\) | \(a_t = a, s_{t+1} = s)\) is the probability of observing \(o\) from state \(s\) after taking action \(a\), \(R(s,a)\) is the reward obtained on taking action \(a\) from state \(s\) and \(\gamma\) is the discount factor. Related Works In partially observable domains, using standard RL algorithms which treat the observations as if they were the states of an underlying Markov decision problem (MDP) can lead to sub-optimal behaviour or in the worst case, the parameters learned by the RL algorithm can fail to converge or even diverge [5, 6]. The theoretical soundness of such algorithms is questionable if the underlying representation of the environment is not fully markov. A large variety of algorithms which perform different kinds of stochastic gradient descent on a fixed error function have been developed [7, 8, 9]. These usually condition the action choice on immediate observation plus the state of an internal memory. Empirical evidence suggests that these are much slower than the memory-less RL algorithms because of the added complexity of storing and updating history or some sort of belief states. The work of [2] on this MCESP algorithm differs from the general approach in this domain by defining a novel action value function which allows us to integrate exploring starts with policy search while still enjoying strong convergence properties. Our contribution of extension of the MCESP algorithm to stochastic policies and the empirical verification of its dominance over standard memory-less approaches like the work done by [10], to the best of our knowledge, is new. MCESP Note: Refer to Perkins 2002 [2] for more detials of deterministic MCESP. Neighbouring Policy One of the key aspect of MCESP algorithm is its interpretation as a local search algorithm in the space of memory-less policies. In that regard, for a policy \(\pi\), define its neighbouring policy as \(\pi \leftarrow (o, a)\) that is identical to \(\pi\) except that it assigns action \(a\) corresponding to observation \(o\). Action Value Function For a trajectory \(\tau\), the discounted episodic reward is defined as \(R(\tau) = \sum_{i=0}^{\infty}\gamma^ir_i\). Let an observation \(o\) occurs in trajectory \(\tau\) for the first time at time step \(j\), then we define \(R_{pre-o}(\tau) = \sum_{i=0}^{j-1}\gamma^ir_i\) and \(R_{post-o}(\tau) = \sum_{i=j}^{\infty}\gamma^ir_i\) i.e. the parts of the discounted episodic reward before and after the first occurrence of observation \(o\) respectively. Following this, we define the action value function as the expected post observation return of the neighbouring policy, i.e. \[Q^{\pi}_{o,a} = E^{\pi \leftarrow (o,a)}\{R_{post-o(\tau)}\}\] This definition of action value differs from the one in a standard MDP case in three aspects: The notion of a staring state is replaced by the first occurrence of an observation \(o\) After taking the immediate action \(a\), the agent follows a neighbouring policy \(\pi \leftarrow (o,a)\) of policy \(\pi\) The action value function, instead of being the discounted return that follows \(o\), it is the portion of discounted reward following observation \(o\) This definition of action value function preserves (to some degree) the property of MDP’s that an optimal is necessarily greedy with respect to its action values. Theorem 1 For all \(\pi\) and \(\pi&apos;=\pi \leftarrow (o,a)\) \[V^{\pi} + \epsilon \geq V^{\pi&apos;} \iff Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a}\] Proof \[V^{\pi} + \epsilon \geq V^{\pi&apos;}\] \[\iff E^{\pi}\{R_{pre-o(\tau)}\} + E^{\pi}\{R_{post-o(\tau)}\} + \epsilon\] \[\geq E^{\pi&apos;}\{R_{pre-o(\tau)}\} + E^{\pi&apos;}\{R_{post-o(\tau)}\}\] Now, as the expected discounted reward before first occurrence of observation \(o\) is independent of the action taken from \(o \implies E^{\pi}\{R_{pre-o(\tau)}\} = E^{\pi \leftarrow (o,a)}\{R_{pre-o(\tau)}\}\) \[\iff E^{\pi}\{R_{post-o(\tau)}\} + \epsilon \geq E^{\pi \leftarrow (o,a)}\{R_{post-o(\tau)}\}\] \[\iff Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a}\] Following these definitions, we say that a policy \(\pi\) is an \(\epsilon\) locally optimal policy if and only if its action value is at-least \(\epsilon\) beter than all its neighbouring policies, i.e. it satisfies \(Q^{\pi}_{o,\pi(o)} + \epsilon \geq Q^{\pi}_{o,a} \forall\) observations \(o\) and actions \(a\). Algorithm The algorithm itself can be seen as two independent parts, first between lines (4-7) and second between lines (8-12). Since the environment is only partially observable to the agent, given a policy, the action values themselves are an estimate of the true values as they are approximated using discounted reward of a finite number of trajectories. The first part of the algorithm works on bringing these action value estimates closer and closer to their true values by performing first visit Monte-Carlo updates for each selected pair of observations and actions. Based on the present estimates, the second part of the algorithm performs a policy search and tries to find neighbouring policy which is significantly better (at-least \(\epsilon\) better) than the current policy. This threshold is dependent on the number of policy updates (n) and the number of action-value updates for the selected observation-action pair (\(c_{o,a}\)) after the last policy update. Based on different observation-action pair selection methods (uniformly at random or in round robin fashion), different learning rate schedules (\(\alpha\)) and different choices for comparison threshold (\(\epsilon\)) functions, different variants of MCSEP can be obtained which differ in their performance, computational complexity and convergence properties. We present three such variations each one of which integrate the MCESP algorithm to incorporate different ideas from stochatic optimisation literature. MCESP SAA If the dynamics of the POMDP are unknown to the agent, exact evaluation of any policy is not possible. By generating a fixed number of trajectories(k) under a given policy, an estimate of the policy’s value can be obtained as the average discounted return. This approach is known as Sample Average Approximation [11] in stochastic optimization literature. For larger values of k, the action-value estimates are expected to be accurate and the algorithm is expected to converge to the local-optimal solution. \(\alpha(n,i) = \dfrac{1}{i + 1}\) and \(\epsilon(n,i,j) = \begin{cases} \infty &amp;amp; i &amp;lt; k \hspace{2mm} or \hspace{2mm} j &amp;lt; k \\ 0 &amp;amp; otherwise \end{cases}\) MCESP PALO In finite local neighborhood structure, PALO [12] algorithm is a general method for hill-climbing in the solution space of a stochastic optimization problem. In MCESP-PALO, the decaying learning rate corresponds to simple averaging, and the comparison threshold is based on Hoeffding’s inequality. The algorithm strictly moves towards the local optimal policy and is expected to converge with absolute certainty. \[\epsilon(n,i,j) = \begin{cases} (y-x)\sqrt{\dfrac{1}{2i}ln\bigg(\dfrac{2(k_n-1)N)}{\delta_n}\bigg)} &amp;amp; i = j &amp;lt; k_n \\ \dfrac{\epsilon}{2} &amp;amp; i = j = k_n \\ \infty &amp;amp; otherwise\\ \end{cases}\] \(k_n = \left\lceil {2\dfrac{(y-x)^{2}}{\epsilon^{2}} ln\Big(\dfrac{2N}{\delta_{n}}\Big)}\right\rceil\) , \(\delta_{n} = \dfrac{6\delta}{n^{2}\pi^{2}}\) , \(\alpha(n,i) = \dfrac{1}{i + 1}\) MCESP CE Since the action values are stochastic, after any finite number of steps, it is not possible to achieve local optimal policy with absolute certainty. However, it serves good as an indicator of non-optimal policy and thereby suggesting a switch. This ensures that the algorithm with a constant comparison threshold will surely converge to at least a \(\epsilon\)-locally optimal policy. Here, \(\alpha(n,i) = b \times i^{-p}\) and \(\epsilon(n,i,j) = \epsilon_0\) MCESP-SAA and MCESP-PALO are much more heavier in terms of computational complexity than MCESP-CE. So, due to resource constraints, we kept our focus on producing results for the constant-epsilon version of our algorithm. We took a couple of tasks (one episodic and one continuing) for testing the theoretical claims and verifying their correctness in general. In both of these, the state of the environment was only partially observable to the agent. Various Partially Observable Domains Parr and Russell’s Grid World Parr and Russell’s Grid World [13] is a \(4\times 3\) grid consisting of 11 states and an obstacle state as shown in Figure 1. The environment is only partially observable to the agent as it can sense only the presence of walls to its immediate east and west and whether it is in the goal (+1) or the penalty (-1) state. The agent can take 4 actions for movement in North, South, East and West directions and its moves in the desired direction with probability 0.8 and slips to either sides with probability 0.1 each. A constant reward of -0.04 is received by the agent for each transition except for the transition to terminal states, where it receives +1 for reaching goal state and -1 for reaching penalty state. In this partially observable setting, the agent is required to learn the best memory-less policy which maximises the discounted episodic reward from the start state to the terminal state. Chrisman’s Space Shuttle Docking Problem Chrisman’s Space Shuttle Docking Problem [14] comprises of two stations as shown in Figure 2. The stations are observable to the agent as least recently visited (LRV) or most recently visited (MRV) based on the most recent docking of the shuttle to one of the stations. The agent can sense only 5 different observations in this 8 state POMDP. It can observe as being in front of or being docked to one of the stations(MRV or LRV), and seeing nothing in front. Out of the three actions that agent can take, two (go-forward and turn-around) are deterministic whereas action backup succeed with probability less than 1. Action backup has different effects based on the true state of the shuttle. If the true state of the shuttle is the middle space, then action backup succeeds with probability 0.8, no effect with probability 0.1 and the effect of turn-around with probability 0.1. If the shuttle is in front of one of the stations with its back facing towards it, action backup will result in successful docking with probability 0.7 and no effect with probability 0.3. If the shuttle is already docked into one of the stations, action backup will cause the the station to deterministically change to the MRV station. If the shuttle is facing one of the stations, then action backup results in successful docking with probability 0.3, results in no effect with probability 0.4 and the effect of turn-around with probability 0.3. As the name suggests, action go-forward propels the shuttle in the forward direction and action turn-around rotates the shuttle by 180 degrees, i.e. reverse the direction of the shuttle. A reward of +10 is awarded for successful docking into the LRV (using action backup), a penalty of -3 is incurred for bumping into the station (taking action go-forward while facing the station). A constant penalty of -0.004 is incurred to incentivise the agent to perform the task as quickly as possible. The goal for the agent is to learn a memory-less deterministic policy that continuously docks to the two stations in an alternate manner. Stochastic MCESP The dominance of stochastic policies over deterministic ones has been empirically proven in a large number of independent works and is widely accepted in pomdp literature with respect to memory-less policies. In this regard, we modified the MCESP algorithm to perform the policy search in the continuous space of stochastic policies while still maintaining the core structure of the algorithm. It differs from the deterministic version in two key aspects. First, the neighbouring policy \(\pi \leftarrow (o,a)\) is now defined on the basis of a little perturbation (\(\delta\)) in the conditional probability for an action \(a\) given an observation \(o\). And second, the policy search step is now based on the comparison between the expected values of the action value functions. where \(\pi \leftarrow (o,a) = Normalised(\pi&apos;)\) and \(\pi&apos;(a&apos;|o&apos;) = \begin{cases} \hspace{10pt} \pi(a&apos;|o&apos;) &amp;amp; if \hspace{7pt} (o&apos;,a&apos;) \neq (o,a)\\ \pi(a&apos;|o&apos;)+\delta &amp;amp; \hspace{10pt} otherwise\\ \end{cases}\) Results The plots below shows the comparison between the deterministic algorithm (MCESP-CS), the stochastic algorithm (Williams and Singh [10]) and our modified algorithm (Stochastic MCESP) for Parr and Russell Grid World and Chrisman’s Shuttle problem. We can see from both the plots that our modified algorithm dominates both the deterministic version of MCESP and the algorithm by Williams and Singh in terms of value of the final optimal policy when restricted to same amount of training (by fixing the number of interactions with the environment). We can observe that the algorithm by Williams and Singh achieves convergence earlier, but it is nowhere near to the optimal value to which stochastic MCESP converges. Also, the amount of tuning and time taken by Willams and Singh’s is much more than by both the versions of MCESP. Conclusion We begin with presenting a novel algorithm MCESP, a model-free memory-less reinforcement learning algorithm for POMDP’s. We ran different variants of the algorithm on various benchmarks [15] in partially observable domains and verify the theoretical claims and convergence properties. The major drawback in our algorithm is that a lot of times, we found that the algorithm is converging to a locally and not globally optimal policy. That is the trade off that the algorithm has in order to enjoy such theoretical guarantees. We focused on one episodic and one continuous task. This can be further extended to see how these claims generalize for much more complex domains. Since we know that memory-less stochastic policies in general work much better than deterministic ones for POMDP’s, we modified our algorithm so that now it performs the policy search in the space of stochastic policies without perturbing the core structure of the algorithm. We then compare this with the deterministic version and other stochastic learning algorithms to verify its superior performance. Still, our work is by no means completely refined in all its aspects. Several improvements might be possible like using complex strategies for exploration techniques instead of simple round-robin or selection observation-action uniformly at random. References 1. S. Sutton, R., and G. Barto, A. 2014. Reinforcement learning: An introduction second edition: 2014, 2015. 2. Perkins, T. J. 2002. Reinforcement learning for pomdps based on action values and stochastic optimization. 199–204. 3. Gordon, G. J. 1996. Chattering in sarsa(lambda) - a cmu learning lab internal report. 4. Pendrith, M. D., and Mcgarity, M. J. 1998. An analysis of direct reinforcement learning in non-markovian domains. 421–429. 5. Whitehead, S. 1992. Reinforcement learning for the adaptive control of perception and action. 6. Baird, L. 1995. Residual algorithms: Reinforcement learning with function approximation. 30–37. 7. Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256. 8. Baird, L., and Moore, A. 1998. Gradient descent for general reinforcement learning. 968–974. 9. Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.; et al. 2000. Policy gradient methods for reinforcement learning with function approximation. 99:1057–1063. 10. Williams, J. K., and Singh, S. 1999. Experimental results on learning stochastic memoryless policies for partially observable markov decision processes. 1073–1079. 11. Kleywegt, A. J.; Shapiro, A.; and Homem-de Mello, T. 2002. The sample average approximation method for stochastic discrete optimization. SIAM J. on Optimization 12(2):479–502. 12. Greiner, R. 1995. Palo: A probabilistic hillclimbing algorithm. Artificial Intelligence 83:1–2. 13. Parr, R., and Russell, S. 1995. Approximating optimal policies for partially observable stochastic domains. 95:1088–1094. 14. Chrisman, L. 1992. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. 1992:183–188 15. Cassandra, A. R. 2003. Pomdp examples. Available at pomdp.org To cite this article: @article{smcesp2021, title={Stochastic Monte Carlo Exploration Starts for POMDPs}, author={Dahale, Rishabh and Tongia, Nitish}, year={2021} }</summary></entry><entry><title type="html">GNU Radio - Embedded Python Block</title><link href="https://rishabhdahale.github.io/blog/2021/gnu-radio/" rel="alternate" type="text/html" title="GNU Radio - Embedded Python Block" /><published>2021-12-28T16:40:00+00:00</published><updated>2021-12-28T16:40:00+00:00</updated><id>https://rishabhdahale.github.io/blog/2021/gnu-radio</id><content type="html" xml:base="https://rishabhdahale.github.io/blog/2021/gnu-radio/">&lt;p&gt;GNU Radio is a open source and free software which provides signal processing blocks to implement software radio. This is a very good software which can do both software simulation and even connect to external hardware. However, I found that the documentation of some of the blocks is not upto the mark (no offense to the developers). One such block is the Embedded Python Block. This is one of the most powerful blocks present in the software as it allows you to do custom calculations. However some very minute details are missing from the official wiki. I had made a doc with details of getting started with this block for one of the graduate level lab at IIT Bombay. In this blog post I’ll be sharing the same.&lt;/p&gt;

&lt;h1&gt;Embedded Python Block&lt;/h1&gt;

&lt;p&gt;GNU Radio provides an Embedded python block to do custom processing of the signals. This block is available under the name of “Python Block”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/gnu_radio/gnu1.png&quot; alt=&quot;GNU Python Block&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By default, this block performs scaling of the signal. To revise the python code for this block follow the following steps:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Double click on the block OR right click and choose properties &lt;/li&gt;
	&lt;li&gt;Select “Open in Editor”
		&lt;ol&gt;
			&lt;li&gt;Note: This will open an prompt asking editor to select. Select editor of your choice&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;This will open a python script with the contents shown in the figure below
		&lt;img src=&quot;/assets/img/posts/gnu_radio/gnu2.png&quot; alt=&quot;GNU Python Block&quot; style=&quot;width:500px;&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;There are 2 functions which you should edit: &lt;i&gt;__init__&lt;/i&gt; and &lt;i&gt;work&lt;/i&gt; of the blk class. Note that this is class is a child of gnuradio’s &lt;i&gt;sync_block&lt;/i&gt; class which will provide some of the functionalities.
	&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;&lt;i&gt;__init__&lt;/i&gt; function&lt;/h2&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;name=&quot;Embedded Python Block&quot;&lt;/strong&gt; is the name that will appear in the GNU radio’s GUI. You can change this name as per your wish&lt;/li&gt;
	&lt;li&gt;in_sig=[np.complex64] defined the input type to the block. You can change this to one of the following type: np.int16 which is basically short data type, np.int32 which is int data type, np.float32 or np.float64 which are float data type and np.complex64. To add these parameters you can simply change the line as follows:&lt;br /&gt;
		&lt;i&gt;in_sig = [np.complex64, np.float32, np.int16]&lt;/i&gt;&lt;br /&gt;
	This will add 3 inputs with the specified input data type &lt;br /&gt;
	&lt;img src=&quot;/assets/img/posts/gnu_radio/gnu3.png&quot; alt=&quot;GNU Python Block&quot; style=&quot;width:200px;&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;out_sig=[np.complex64] works the same was as in_sig but with the output ports&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Adding Parameters:&lt;/strong&gt; To add the parameters to the block, simply add them to the input arguments of the &lt;i&gt;__init__&lt;/i&gt; function. Make sure to add them as class variables if necessary&lt;/li&gt;
	&lt;li&gt;Apart from the above points you can use the &lt;i&gt;__init__&lt;/i&gt; function as a per your wish following the rules of python programming language&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;&lt;i&gt;work&lt;/i&gt; function&lt;/h2&gt;
&lt;p&gt;This is the function where code for processing the inputs and outputs would be written. This function have 2 arguments which should &lt;strong&gt;not&lt;/strong&gt; be changed:&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;input_items&lt;/strong&gt; &amp;amp; &lt;strong&gt;output_items&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;input_items is a list of numpy array of input signals. The inputs will be arranged as they are defined in the in_sig in &lt;strong&gt;init&lt;/strong&gt; function. You can use various numpy functions and their it’s vectorization to your make your code run fast. Output_items follow the same logic with outputs.&lt;br /&gt;
For e.g. below is a simple code which scales the input and clips it’s amplitude&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/posts/gnu_radio/gnu4.png&quot; alt=&quot;GNU Python Block&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The work function returns as output an integer indicating the number of items in the list output_items. By default it returns len(output_items) which is the max length of output items(8192 by default). This means every call to the work function processes a block of input of len(output items) at a time.&lt;/p&gt;

&lt;h2&gt;Debugging Embedded Python Block&lt;/h2&gt;

&lt;p&gt;This is a very tricky task. I could not find log files of GNU radio. It does not show the error in the GUI. If there is an error in your EPB block code, the GNU radio will still create the flow-graph but the output of the EPB block will be taken as a zero vector. To avoid this, the only way I can suggest for debugging for now is to write a separate standalone script for checking the functionalities of your work function.&lt;/p&gt;</content><author><name></name></author><summary type="html">GNU Radio is a open source and free software which provides signal processing blocks to implement software radio. This is a very good software which can do both software simulation and even connect to external hardware. However, I found that the documentation of some of the blocks is not upto the mark (no offense to the developers). One such block is the Embedded Python Block. This is one of the most powerful blocks present in the software as it allows you to do custom calculations. However some very minute details are missing from the official wiki. I had made a doc with details of getting started with this block for one of the graduate level lab at IIT Bombay. In this blog post I’ll be sharing the same. Embedded Python Block GNU Radio provides an Embedded python block to do custom processing of the signals. This block is available under the name of “Python Block”. By default, this block performs scaling of the signal. To revise the python code for this block follow the following steps: Double click on the block OR right click and choose properties Select “Open in Editor” Note: This will open an prompt asking editor to select. Select editor of your choice This will open a python script with the contents shown in the figure below There are 2 functions which you should edit: __init__ and work of the blk class. Note that this is class is a child of gnuradio’s sync_block class which will provide some of the functionalities. __init__ function name=&quot;Embedded Python Block&quot; is the name that will appear in the GNU radio’s GUI. You can change this name as per your wish in_sig=[np.complex64] defined the input type to the block. You can change this to one of the following type: np.int16 which is basically short data type, np.int32 which is int data type, np.float32 or np.float64 which are float data type and np.complex64. To add these parameters you can simply change the line as follows: in_sig = [np.complex64, np.float32, np.int16] This will add 3 inputs with the specified input data type out_sig=[np.complex64] works the same was as in_sig but with the output ports Adding Parameters: To add the parameters to the block, simply add them to the input arguments of the __init__ function. Make sure to add them as class variables if necessary Apart from the above points you can use the __init__ function as a per your wish following the rules of python programming language work function This is the function where code for processing the inputs and outputs would be written. This function have 2 arguments which should not be changed: input_items &amp;amp; output_items input_items is a list of numpy array of input signals. The inputs will be arranged as they are defined in the in_sig in init function. You can use various numpy functions and their it’s vectorization to your make your code run fast. Output_items follow the same logic with outputs. For e.g. below is a simple code which scales the input and clips it’s amplitude The work function returns as output an integer indicating the number of items in the list output_items. By default it returns len(output_items) which is the max length of output items(8192 by default). This means every call to the work function processes a block of input of len(output items) at a time. Debugging Embedded Python Block This is a very tricky task. I could not find log files of GNU radio. It does not show the error in the GUI. If there is an error in your EPB block code, the GNU radio will still create the flow-graph but the output of the EPB block will be taken as a zero vector. To avoid this, the only way I can suggest for debugging for now is to write a separate standalone script for checking the functionalities of your work function.</summary></entry><entry><title type="html">My First Blog Post</title><link href="https://rishabhdahale.github.io/blog/2021/first-post/" rel="alternate" type="text/html" title="My First Blog Post" /><published>2021-11-24T16:40:16+00:00</published><updated>2021-11-24T16:40:16+00:00</updated><id>https://rishabhdahale.github.io/blog/2021/first-post</id><content type="html" xml:base="https://rishabhdahale.github.io/blog/2021/first-post/">&lt;p&gt;Hello there. I’m Rishabh Dahale. When writing this post, I’m a final year undergraduate student at the Department of Electrical Engineering at IIT Bombay. I’m pursuing a Dual Degree program with a specialization in Communication and Signal Processing. For my final year thesis project, I’m working under &lt;a href=&quot;https://www.ee.iitb.ac.in/web/people/faculty/home/prao&quot;&gt;Prof. Preeti Rao&lt;/a&gt; on Automatic-Music Generation using Reinforcement Learning. 
Through this blog, I intend to share my projects and, at times, some other interesting research papers and topics I come across. You can fork the original repo from &lt;a href=&quot;https://github.com/alshedivat/al-folio&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is a short post linked to a colab notebook with image style transfer using neural network code. This is because if you check one of my picture &lt;a href=&quot;/assets/img/prof_pic.png&quot;&gt;here&lt;/a&gt;, you will notice it’s edited. The catch is that I didn’t apply the filter on my own or use very sophisticated software like adobe lightroom or anything for it. Instead, I simply gave my image and another image whose style I wanted on my picture to a neural network. More specifically, I have used the method proposed in &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf&quot;&gt;this paper&lt;/a&gt;. You can find the code for this implementation in this &lt;a href=&quot;https://github.com/RishabhDahale/Image-Style-Transfer-using-CNN&quot;&gt;github repo&lt;/a&gt;. Some might say it’s a bit old method for 2021, and more robust methods are published, so why not use them. Well, I implemented this about 2 years ago and had the code with me. And moreover, I use google colab for running such intensive operations. Newer methods would surely give better results but with added time cost.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1Z170lOzHUs8ZGCcnI3_HfF8mKjsXWvia?usp=sharing&quot;&gt;&lt;strong&gt;This is the link&lt;/strong&gt;&lt;/a&gt; to the colab notebook for the same if you want to run it. You will have to copy the notebook before running it, as I have shared the view-only version.&lt;/p&gt;

&lt;p&gt;Can’t wait to share my thoughts through this blog!&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello there. I’m Rishabh Dahale. When writing this post, I’m a final year undergraduate student at the Department of Electrical Engineering at IIT Bombay. I’m pursuing a Dual Degree program with a specialization in Communication and Signal Processing. For my final year thesis project, I’m working under Prof. Preeti Rao on Automatic-Music Generation using Reinforcement Learning. Through this blog, I intend to share my projects and, at times, some other interesting research papers and topics I come across. You can fork the original repo from this link. This is a short post linked to a colab notebook with image style transfer using neural network code. This is because if you check one of my picture here, you will notice it’s edited. The catch is that I didn’t apply the filter on my own or use very sophisticated software like adobe lightroom or anything for it. Instead, I simply gave my image and another image whose style I wanted on my picture to a neural network. More specifically, I have used the method proposed in this paper. You can find the code for this implementation in this github repo. Some might say it’s a bit old method for 2021, and more robust methods are published, so why not use them. Well, I implemented this about 2 years ago and had the code with me. And moreover, I use google colab for running such intensive operations. Newer methods would surely give better results but with added time cost. This is the link to the colab notebook for the same if you want to run it. You will have to copy the notebook before running it, as I have shared the view-only version. Can’t wait to share my thoughts through this blog!</summary></entry></feed>